{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def config_spark():\n",
    "#     \"\"\"\n",
    "#     Configure Spark\n",
    "#     \"\"\"\n",
    "#     import pyspark\n",
    "#     import pyspark.sql.functions as F\n",
    "#     import pyspark.sql.types as T\n",
    "#     spark = pyspark.sql.SparkSession.builder.master('local').getOrCreate()\n",
    "    \n",
    "\n",
    "def to_pandas(sparkDf, n=10):\n",
    "    \"\"\"\n",
    "    Returns a Pandas dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    n: int / float\n",
    "        Top n rows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Pandas dataframe\n",
    "    \"\"\"\n",
    "    pdDf = sparkDf.limit(n).toPandas()\n",
    "    return pdDf\n",
    "\n",
    "\n",
    "def group_count_percent(sparkDf, cols, n=10, dfType='pandas'):\n",
    "    \"\"\"\n",
    "    Returns a Pandas dataframe group by column(s), sort in descending order, calculate count and percent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    cols: str / list\n",
    "        Accepts a (list of) string(s) of column(s)\n",
    "    n: int / float\n",
    "        Top n rows\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A Pandas dataframe\n",
    "    \"\"\"\n",
    "    df = sparkDf.groupBy(cols).count().orderBy('count', ascending=False)\n",
    "    rowCount = sparkDf.count()\n",
    "    \n",
    "    if n == float('inf'):\n",
    "        df = df.withColumn('percent', F.round(F.udf(lambda x: x*100/rowCount)('count'), 3))\n",
    "    else:\n",
    "        df = df.withColumn('percent', F.round(F.udf(lambda x: x*100/rowCount)('count'), 3)).limit(n)\n",
    "    \n",
    "    if dfType == 'pandas':\n",
    "        pdDf = df.toPandas()\n",
    "        return pdDf\n",
    "    \n",
    "    if dfType == 'spark':\n",
    "        return df\n",
    "\n",
    "\n",
    "def info(sparkDf):\n",
    "    \"\"\"\n",
    "    Display Spark dataframe information\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    print('The dataframe: {0}'.format(type(sparkDf)))\n",
    "    print('Number of columns: {0}'.format(len(sparkDf.columns)))\n",
    "    print('Number of rows: {0}'.format(sparkDf.count()))\n",
    "    sparkDf.printSchema()\n",
    "\n",
    "\n",
    "def rename_columns(sparkDf, cols):\n",
    "    \"\"\"\n",
    "    Rename Spark dataframe column(s)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    cols: dict\n",
    "        A dictionary {oldName: newName} of columns to rename\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Spark dataframe\n",
    "    \"\"\"\n",
    "    df = sparkDf.select([F.col(c).alias(cols.get(c,c)) for c in sparkDf.columns])\n",
    "    return df\n",
    "\n",
    "\n",
    "def columns_statistics(sparkDf, n=10):\n",
    "    \"\"\"\n",
    "    Display Spark dataframe columns' statistics and return 2 lists\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    n: int / float\n",
    "        Top n rows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Lists of null-value and single-value columns. null-value list <= single-value list\n",
    "    \"\"\"\n",
    "    info(sparkDf)\n",
    "    nullValueCols, singleValueCols = [], []\n",
    "    \n",
    "    for column in sparkDf.columns:\n",
    "        df = group_count_percent(sparkDf=sparkDf, cols=column, n=n, dfType='Spark')\n",
    "        print(column)\n",
    "        df.show(n=n)\n",
    "        \n",
    "        if df.count() == 1:\n",
    "            singleValueCols.append(column)\n",
    "            print('!!!!! {0} is a candidate to drop !!!!!\\n\\n'.format(column))\n",
    "        \n",
    "            if not df.first()[0] or df.first()[0].casefold() == 'none' or df.first()[0].casefold():\n",
    "                nullValueCols.append(column)\n",
    "        \n",
    "    print('There are {0} of single value columns, they are: {1}'.format(len(singleValueCols), singleValueCols))\n",
    "    print('There are {0} of null value columns, they are: {1}'.format(len(nullValueCols), nullValueCols))\n",
    "    return nullValueCols, singleValueCols\n",
    "\n",
    "\n",
    "def column_into_list(sparkDf, singleCol):\n",
    "    \"\"\"\n",
    "    Convert a Spark dataframe's column into list\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    singleCol: str\n",
    "        One column in sparkDf\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list\n",
    "    \"\"\"\n",
    "    if col in sparkDf.columns:\n",
    "        LIST = sparkDf.select(singleCol).toPandas()[col].values.tolist()\n",
    "        return LIST\n",
    "\n",
    "\n",
    "def column_into_set(sparkDf, singleCol):\n",
    "    \"\"\"\n",
    "    Convert a Spark dataframe's column into set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    singleCol: str\n",
    "        One column in sparkDf\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A set\n",
    "    \"\"\"\n",
    "    SET = set(column_into_list(sparkDf, singleCol))\n",
    "    return SET\n",
    "\n",
    "\n",
    "def prefix_to_columns(sparkDf, prefix):\n",
    "    \"\"\"\n",
    "    Add prefix Spark dataframe's columns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    prefix: str\n",
    "        Prefix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A Spark dataframe\n",
    "    \"\"\"\n",
    "    df = sparkDf\n",
    "    for column in sparkDf.columns:\n",
    "        if not column.startswith(prefix):\n",
    "            df = df.withColumnRenamed(column, prefix+column)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_dummy_columns(sparkDf, cols, value):\n",
    "    \"\"\"\n",
    "    Add dummy column(s) to Spark dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sparkDf: pyspark.sql.dataframe.DataFrame\n",
    "        One Spark dataframe\n",
    "    cols: list\n",
    "        List of columns\n",
    "    value: str\n",
    "        Default value of the new column(s)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Spark dataframe\n",
    "    \"\"\"\n",
    "    df = sparkDf\n",
    "    dummyCols = set(cols) - set(sparkDf.columns)\n",
    "    for column in dummyCols:\n",
    "        df = df.withColumn(column, F.lit(value))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integers</th>\n",
       "      <th>floats</th>\n",
       "      <th>integer_array</th>\n",
       "      <th>str_array</th>\n",
       "      <th>literal_str_array</th>\n",
       "      <th>literal_str_array2</th>\n",
       "      <th>strs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], ['a'], ['a','b']]</td>\n",
       "      <td>\"[[], [a], [a,b]]\"</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[3, 4, 5]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[[], ['a'], ['a','b']]</td>\n",
       "      <td>\"[[], [a], [a,b]]\"</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>[6, 7, 8, 9]</td>\n",
       "      <td>[a, b]</td>\n",
       "      <td>[[], ['a'], ['a','b']]</td>\n",
       "      <td>\"[[], [a], [a,b]]\"</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integers  floats integer_array str_array       literal_str_array  \\\n",
       "0         1    -1.0        [1, 2]        []  [[], ['a'], ['a','b']]   \n",
       "1         2     0.5     [3, 4, 5]       [a]  [[], ['a'], ['a','b']]   \n",
       "2         3     2.7  [6, 7, 8, 9]    [a, b]  [[], ['a'], ['a','b']]   \n",
       "\n",
       "   literal_str_array2  strs  \n",
       "0  \"[[], [a], [a,b]]\"  null  \n",
       "1  \"[[], [a], [a,b]]\"        \n",
       "2  \"[[], [a], [a,b]]\"  None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.DataFrame(data=\n",
    "                     {'integers': [1, 2, 3], \n",
    "                      'floats': [-1.0, 0.5, 2.7], \n",
    "                      'integer_array': [[1, 2], [3, 4, 5], [6, 7, 8, 9]], \n",
    "                      'str_array': [[], ['a'], ['a','b']], \n",
    "                      'literal_str_array': \"[[], ['a'], ['a','b']]\", \n",
    "                      'literal_str_array2': '\"[[], [a], [a,b]]\"', \n",
    "                      'strs': ['null', '', None]\n",
    "                     })\n",
    "\n",
    "df = spark.createDataFrame(df_pd)\n",
    "to_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>floats</th>\n",
       "      <th>integer_array</th>\n",
       "      <th>str_array</th>\n",
       "      <th>literal_str_array</th>\n",
       "      <th>literal_str_array2</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], ['a'], ['a','b']]</td>\n",
       "      <td>\"[[], [a], [a,b]]\"</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[3, 4, 5]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[[], ['a'], ['a','b']]</td>\n",
       "      <td>\"[[], [a], [a,b]]\"</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>[6, 7, 8, 9]</td>\n",
       "      <td>[a, b]</td>\n",
       "      <td>[[], ['a'], ['a','b']]</td>\n",
       "      <td>\"[[], [a], [a,b]]\"</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer  floats integer_array str_array       literal_str_array  \\\n",
       "0        1    -1.0        [1, 2]        []  [[], ['a'], ['a','b']]   \n",
       "1        2     0.5     [3, 4, 5]       [a]  [[], ['a'], ['a','b']]   \n",
       "2        3     2.7  [6, 7, 8, 9]    [a, b]  [[], ['a'], ['a','b']]   \n",
       "\n",
       "   literal_str_array2  test  \n",
       "0  \"[[], [a], [a,b]]\"  null  \n",
       "1  \"[[], [a], [a,b]]\"        \n",
       "2  \"[[], [a], [a,b]]\"  None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsToRename = {'strs':'test', 'integers':'integer'}\n",
    "\n",
    "to_pandas(rename_columns(df, colsToRename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "?group_count_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = None\n",
    "x = 'None'\n",
    "x = 'null'\n",
    "x = 'Null'\n",
    "\n",
    "not x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = None\n",
    "x = 'None'\n",
    "x = 'null'\n",
    "x = 'Null'\n",
    "\n",
    "'none' in x.casefold()\n",
    "'null' in x.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = ['a','b','c']\n",
    "t1 = ['c','e','f']\n",
    "\n",
    "set(t0) - set(t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
