<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>data_manipulation.pyspark_ &#8212; data_manipulation 0.46 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/nature.css?v=279e0f84" />
    <script src="../../_static/documentation_options.js?v=ef64108f"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">data_manipulation 0.46 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">data_manipulation.pyspark_</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for data_manipulation.pyspark_</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">pyspark</span>


<span class="c1"># CONFIG</span>
<div class="viewcode-block" id="config_spark_local">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.config_spark_local">[docs]</a>
<span class="k">def</span> <span class="nf">config_spark_local</span><span class="p">(</span><span class="n">autoset</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configures Spark for local execution with optimized settings based on system resources.</span>

<span class="sd">    Automatically calculates and sets optimal Spark configuration parameters based on:</span>
<span class="sd">    - Available CPU cores</span>
<span class="sd">    - System memory</span>
<span class="sd">    - Executor allocation</span>
<span class="sd">    - Memory distribution</span>

<span class="sd">    Args:</span>
<span class="sd">        autoset (bool, optional): Whether to automatically apply the configuration.</span>
<span class="sd">            If False, only prints recommended settings. Defaults to True.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; config_spark_local()</span>
<span class="sd">        Here is the current computer specs ...</span>
<span class="sd">        executor_per_node: 1</span>
<span class="sd">        spark_executor_instances: 1</span>
<span class="sd">        total_executor_memory: 30</span>
<span class="sd">        spark_executor_memory: 27</span>
<span class="sd">        memory_overhead: 3</span>
<span class="sd">        spark_default_parallelism: 10</span>
<span class="sd">        spark.sql.execution.arrow.pyspark.enabled recommended by Koalas ...</span>
<span class="sd">        spark auto-configured ...</span>

<span class="sd">    Note:</span>
<span class="sd">        Configuration includes:</span>
<span class="sd">        - Executor cores and memory</span>
<span class="sd">        - Driver cores and memory</span>
<span class="sd">        - Memory overhead</span>
<span class="sd">        - Default parallelism</span>
<span class="sd">        - Shuffle partitions</span>
<span class="sd">        - Arrow optimization for PySpark</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">round_down_or_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Here is the  current computer specs ...&quot;</span><span class="p">)</span>
    <span class="n">vcore_per_node</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
    <span class="n">spark_executor_cores</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">number_of_nodes</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">total_ram_per_node_gb</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">sysconf</span><span class="p">(</span><span class="s2">&quot;SC_PAGE_SIZE&quot;</span><span class="p">)</span> <span class="o">*</span> <span class="n">os</span><span class="o">.</span><span class="n">sysconf</span><span class="p">(</span><span class="s2">&quot;SC_PHYS_PAGES&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1024.0</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">executor_per_node</span> <span class="o">=</span> <span class="n">round_down_or_one</span><span class="p">((</span><span class="n">vcore_per_node</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">spark_executor_cores</span><span class="p">)</span>
    <span class="n">spark_executor_instances</span> <span class="o">=</span> <span class="n">round_down_or_one</span><span class="p">(</span>
        <span class="p">(</span><span class="n">executor_per_node</span> <span class="o">*</span> <span class="n">number_of_nodes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">total_executor_memory</span> <span class="o">=</span> <span class="n">round_down_or_one</span><span class="p">(</span>
        <span class="p">(</span><span class="n">total_ram_per_node_gb</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">executor_per_node</span>
    <span class="p">)</span>
    <span class="n">spark_executor_memory</span> <span class="o">=</span> <span class="n">round_down_or_one</span><span class="p">(</span><span class="n">total_executor_memory</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">memory_overhead</span> <span class="o">=</span> <span class="n">round_down_or_one</span><span class="p">(</span><span class="n">total_executor_memory</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">spark_default_parallelism</span> <span class="o">=</span> <span class="n">round_down_or_one</span><span class="p">(</span>
        <span class="n">spark_executor_instances</span> <span class="o">*</span> <span class="n">spark_executor_cores</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;executor_per_node: </span><span class="si">{</span><span class="n">executor_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;spark_executor_instances: </span><span class="si">{</span><span class="n">spark_executor_instances</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;total_executor_memory: </span><span class="si">{</span><span class="n">total_executor_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;spark_executor_memory: </span><span class="si">{</span><span class="n">spark_executor_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;memory_overhead: </span><span class="si">{</span><span class="n">memory_overhead</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;spark_default_parallelism: </span><span class="si">{</span><span class="n">spark_default_parallelism</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">autoset</span><span class="p">:</span>
        <span class="n">spark</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.executor.cores&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">spark_executor_cores</span><span class="p">))</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.cores&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">spark_executor_cores</span><span class="p">))</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.executor.instances&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">spark_executor_instances</span><span class="p">))</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.executor.memory&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">spark_executor_memory</span><span class="si">}</span><span class="s2">g&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.memory&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">spark_executor_memory</span><span class="si">}</span><span class="s2">g&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.executor.memoryOverhead&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">memory_overhead</span><span class="si">}</span><span class="s2">g&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.default.parallelism&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">spark_default_parallelism</span><span class="p">))</span>
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">spark_default_parallelism</span><span class="p">))</span>
            <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;spark.sql.execution.arrow.pyspark.enabled recommended by Koalas ...&quot;</span><span class="p">)</span>
        <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.execution.arrow.pyspark.enabled&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.sql.execution.arrow.pyspark.enabled&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;spark auto-configured ...&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Here is the recommended command to execute:&quot;</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        spark = pyspark.sql.SparkSession.builder.master(&quot;local&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.executor.cores&quot;, &quot;</span><span class="si">{</span><span class="n">spark_executor_cores</span><span class="si">}</span><span class="s2">&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.driver.cores&quot;, &quot;</span><span class="si">{</span><span class="n">spark_executor_cores</span><span class="si">}</span><span class="s2">&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.executor.instances&quot;, &quot;</span><span class="si">{</span><span class="n">spark_executor_instances</span><span class="si">}</span><span class="s2">&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.executor.memory&quot;, &quot;</span><span class="si">{</span><span class="n">spark_executor_memory</span><span class="si">}</span><span class="s2">g&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.driver.memory&quot;, &quot;</span><span class="si">{</span><span class="n">spark_executor_memory</span><span class="si">}</span><span class="s2">g&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.executor.memoryOverhead&quot;, &quot;</span><span class="si">{</span><span class="n">memory_overhead</span><span class="si">}</span><span class="s2">g&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.default.parallelism&quot;, &quot;</span><span class="si">{</span><span class="n">spark_default_parallelism</span><span class="si">}</span><span class="s2">&quot;) </span><span class="se">\</span>
<span class="s2">            .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;</span><span class="si">{</span><span class="n">spark_default_parallelism</span><span class="si">}</span><span class="s2">&quot;) </span><span class="se">\</span>
<span class="s2">            .getOrCreate()</span>
<span class="s2">        &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;config_spark_local exited ...&quot;</span><span class="p">)</span></div>



<span class="c1"># COLUMNS</span>
<div class="viewcode-block" id="add_dummy_columns">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.add_dummy_columns">[docs]</a>
<span class="k">def</span> <span class="nf">add_dummy_columns</span><span class="p">(</span>
    <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">columns</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">value</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds new columns with default values to a Spark DataFrame.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        columns (List[str]): List of column names to add</span>
<span class="sd">        value (str): Default value for the new columns</span>

<span class="sd">    Returns:</span>
<span class="sd">        pyspark.sql.DataFrame: DataFrame with added columns</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If arguments are not of correct type</span>
<span class="sd">            - dataframe must be a Spark DataFrame</span>
<span class="sd">            - columns must be a list</span>
<span class="sd">            - value must be a string</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;Alice&quot;, 1)], [&quot;name&quot;, &quot;id&quot;])</span>
<span class="sd">        &gt;&gt;&gt; new_df = add_dummy_columns(df, [&quot;age&quot;, &quot;city&quot;], &quot;unknown&quot;)</span>
<span class="sd">        &gt;&gt;&gt; new_df.show()</span>
<span class="sd">        +-----+---+---+------+</span>
<span class="sd">        | name| id|age|  city|</span>
<span class="sd">        +-----+---+---+------+</span>
<span class="sd">        |Alice|  1|unknown|unknown|</span>
<span class="sd">        +-----+---+---+------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a list ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a str ...&quot;</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">dataframe</span>
    <span class="n">dummy_columns</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">dummy_columns</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lit</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">df</span></div>



<div class="viewcode-block" id="column_into_list">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.column_into_list">[docs]</a>
<span class="k">def</span> <span class="nf">column_into_list</span><span class="p">(</span><span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extracts values from a DataFrame column into a Python list.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        column (str): Name of the column to extract</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[Any]: List containing all values from the specified column,</span>
<span class="sd">            including duplicates</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dataframe is not a Spark DataFrame or column is not a string</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,), (2,)], [&quot;value&quot;])</span>
<span class="sd">        &gt;&gt;&gt; column_into_list(df, &quot;value&quot;)</span>
<span class="sd">        [1, 2, 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a str ...&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">list_</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">column</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">list_</span></div>



<div class="viewcode-block" id="column_into_set">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.column_into_set">[docs]</a>
<span class="k">def</span> <span class="nf">column_into_set</span><span class="p">(</span><span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extracts unique values from a DataFrame column into a Python set.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        column (str): Name of the column to extract</span>

<span class="sd">    Returns:</span>
<span class="sd">        Set[Any]: Set containing unique values from the specified column</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dataframe is not a Spark DataFrame or column is not a string</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,), (2,)], [&quot;value&quot;])</span>
<span class="sd">        &gt;&gt;&gt; column_into_set(df, &quot;value&quot;)</span>
<span class="sd">        {1, 2}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a str ...&quot;</span><span class="p">)</span>

    <span class="n">set_</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">column_into_list</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">set_</span></div>



<div class="viewcode-block" id="columns_prefix">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.columns_prefix">[docs]</a>
<span class="k">def</span> <span class="nf">columns_prefix</span><span class="p">(</span>
    <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds a prefix to all column names in a DataFrame.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        prefix (str): Prefix to add to column names</span>

<span class="sd">    Returns:</span>
<span class="sd">        pyspark.sql.DataFrame: DataFrame with renamed columns</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dataframe is not a Spark DataFrame or prefix is not a string</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;Alice&quot;, 1)], [&quot;name&quot;, &quot;id&quot;])</span>
<span class="sd">        &gt;&gt;&gt; new_df = columns_prefix(df, &quot;user_&quot;)</span>
<span class="sd">        &gt;&gt;&gt; new_df.show()</span>
<span class="sd">        +---------+-------+</span>
<span class="sd">        |user_name|user_id|</span>
<span class="sd">        +---------+-------+</span>
<span class="sd">        |    Alice|      1|</span>
<span class="sd">        +---------+-------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a str ...&quot;</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">dataframe</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">column</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">):</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">column</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>



<div class="viewcode-block" id="columns_statistics">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.columns_statistics">[docs]</a>
<span class="k">def</span> <span class="nf">columns_statistics</span><span class="p">(</span>
    <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Analyzes column statistics and identifies empty and single-value columns.</span>

<span class="sd">    Performs comprehensive analysis of each column including:</span>
<span class="sd">    - Value counts</span>
<span class="sd">    - Empty value detection</span>
<span class="sd">    - Single value detection</span>
<span class="sd">    - Basic statistics</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        n (int, optional): Number of top values to display for each column.</span>
<span class="sd">            Defaults to 10.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[List[str], List[str]]: Two lists containing:</span>
<span class="sd">            - List of empty column names</span>
<span class="sd">            - List of single-value column names</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dataframe is not a Spark DataFrame</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([</span>
<span class="sd">        ...     (&quot;Alice&quot;, None),</span>
<span class="sd">        ...     (&quot;Alice&quot;, None)</span>
<span class="sd">        ... ], [&quot;name&quot;, &quot;email&quot;])</span>
<span class="sd">        &gt;&gt;&gt; empty_cols, single_cols = columns_statistics(df)</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;Empty columns: {empty_cols}&quot;)</span>
<span class="sd">        Empty columns: [&#39;email&#39;]</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;Single value columns: {single_cols}&quot;)</span>
<span class="sd">        Single value columns: [&#39;name&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>

    <span class="n">describe</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
    <span class="n">empty_columns</span><span class="p">,</span> <span class="n">single_columns</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">group_count</span><span class="p">(</span><span class="n">dataframe</span><span class="o">=</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">single_columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;!!!!! </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2"> is a candidate to drop !!!!!</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="n">df</span><span class="o">.</span><span class="n">first</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="ow">or</span> <span class="n">df</span><span class="o">.</span><span class="n">first</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">casefold</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span>
                <span class="ow">or</span> <span class="n">df</span><span class="o">.</span><span class="n">first</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">casefold</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">empty_columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">single_columns</span><span class="p">)</span><span class="si">}</span><span class="s2"> of single value columns, they are: </span><span class="si">{</span><span class="n">single_columns</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">empty_columns</span><span class="p">)</span><span class="si">}</span><span class="s2"> of null value columns, they are: </span><span class="si">{</span><span class="n">empty_columns</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">empty_columns</span><span class="p">,</span> <span class="n">single_columns</span></div>



<span class="c1"># DATAFRAME</span>
<div class="viewcode-block" id="rename">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.rename">[docs]</a>
<span class="k">def</span> <span class="nf">rename</span><span class="p">(</span>
    <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">columns</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Renames multiple columns in a DataFrame using a mapping dictionary.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        columns (Dict[str, str]): Dictionary mapping old column names to new names</span>

<span class="sd">    Returns:</span>
<span class="sd">        pyspark.sql.DataFrame: DataFrame with renamed columns</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dataframe is not a Spark DataFrame or columns is not a dict</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;Alice&quot;, 1)], [&quot;_1&quot;, &quot;_2&quot;])</span>
<span class="sd">        &gt;&gt;&gt; new_df = rename(df, {&quot;_1&quot;: &quot;name&quot;, &quot;_2&quot;: &quot;id&quot;})</span>
<span class="sd">        &gt;&gt;&gt; new_df.show()</span>
<span class="sd">        +-----+---+</span>
<span class="sd">        | name| id|</span>
<span class="sd">        +-----+---+</span>
<span class="sd">        |Alice|  1|</span>
<span class="sd">        +-----+---+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a dict ...&quot;</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
        <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="n">columns</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>



<span class="c1"># STATISTICS</span>
<div class="viewcode-block" id="describe">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.describe">[docs]</a>
<span class="k">def</span> <span class="nf">describe</span><span class="p">(</span><span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prints comprehensive information about a DataFrame.</span>

<span class="sd">    Displays:</span>
<span class="sd">    - DataFrame type</span>
<span class="sd">    - Number of columns</span>
<span class="sd">    - Number of rows</span>
<span class="sd">    - Schema information</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dataframe is not a Spark DataFrame</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;Alice&quot;, 1)], [&quot;name&quot;, &quot;id&quot;])</span>
<span class="sd">        &gt;&gt;&gt; describe(df)</span>
<span class="sd">        The dataframe: &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;</span>
<span class="sd">        Number of columns: 2</span>
<span class="sd">        Number of rows: 1</span>
<span class="sd">        root</span>
<span class="sd">         |-- name: string (nullable = true)</span>
<span class="sd">         |-- id: long (nullable = true)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The dataframe: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of columns: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of rows: </span><span class="si">{</span><span class="n">dataframe</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">dataframe</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span></div>



<div class="viewcode-block" id="group_count">
<a class="viewcode-back" href="../../data_manipulation.html#data_manipulation.pyspark_.group_count">[docs]</a>
<span class="k">def</span> <span class="nf">group_count</span><span class="p">(</span>
    <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">columns</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">n</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Performs group by operation and calculates count and percentage for each group.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pyspark.sql.DataFrame): Input Spark DataFrame</span>
<span class="sd">        columns (Union[str, List[str]]): Column(s) to group by</span>
<span class="sd">        n (Union[int, float], optional): Number of top groups to return.</span>
<span class="sd">            Use float(&#39;inf&#39;) for all groups. Defaults to 10.</span>

<span class="sd">    Returns:</span>
<span class="sd">        pyspark.sql.DataFrame: DataFrame with columns:</span>
<span class="sd">            - Group by column(s)</span>
<span class="sd">            - count: Count of records in each group</span>
<span class="sd">            - percent: Percentage of total records in each group</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If arguments are not of correct type</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([</span>
<span class="sd">        ...     (1, &#39;A&#39;), (1, &#39;B&#39;), (2, &#39;A&#39;)</span>
<span class="sd">        ... ], [&quot;id&quot;, &quot;category&quot;])</span>
<span class="sd">        &gt;&gt;&gt; group_count(df, [&quot;id&quot;]).show()</span>
<span class="sd">        +---+-----+-------+</span>
<span class="sd">        | id|count|percent|</span>
<span class="sd">        +---+-----+-------+</span>
<span class="sd">        |  1|    2|   66.7|</span>
<span class="sd">        |  2|    1|   33.3|</span>
<span class="sd">        +---+-----+-------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a Pyspark dataframe ...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument must be a list ...&quot;</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">row_count</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span>
        <span class="s2">&quot;percent&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="n">row_count</span><span class="p">)(</span><span class="s2">&quot;count&quot;</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">):</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>



<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">doctest</span>

    <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">()</span>
</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">data_manipulation 0.46 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">data_manipulation.pyspark_</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright .
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </div>
  </body>
</html>